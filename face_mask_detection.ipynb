{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Face Mask Detection using a Convolutional Neural Network (CNN)\n",
        "\n",
        "This notebook walks through the process of building a CNN model to classify whether a person in an image is wearing a face mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step1"
      },
      "source": [
        "### Step 1: Setup and Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# kaggle installation\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up the Kaggle API credentials.\n",
        "# This is just boilerplate to get our kaggle.json file in the right place.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the dataset from Kaggle.\n",
        "!kaggle datasets download -d omkargurav/face-mask-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from zipfile import ZipFile\n",
        "dataset = '/content/face-mask-dataset.zip'\n",
        "\n",
        "with ZipFile(dataset,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Dataset extraction successful!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step2"
      },
      "source": [
        "### Step 2: Import Libraries and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing all the good stuff we need for this project.\n",
        "# Standard libraries for data handling, plotting, and of course, deep learning.\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting a list of all the image filenames from both directories.\n",
        "with_mask_files = os.listdir('/content/data/with_mask')\n",
        "without_mask_files = os.listdir('/content/data/without_mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Just a quick check to see how many images we have for each class.\n",
        "print(\"Number of images with mask: \", len(with_mask_files))\n",
        "print(\"Number of images without mask: \", len(without_mask_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step3"
      },
      "source": [
        "### Step 3: Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating numerical labels for our two classes.\n",
        "# This is how the model will understand the difference.\n",
        "# With Mask --> 1\n",
        "# Without Mask --> 0\n",
        "with_mask_labels = [1] * len(with_mask_files)\n",
        "without_mask_labels = [0] * len(without_mask_files)\n",
        "\n",
        "# Combining the labels into a single list.\n",
        "labels = with_mask_labels + without_mask_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Looping through each image, resizing it to 128x128, and converting it into a NumPy array.\n",
        "with_mask_path = '/content/data/with_mask/'\n",
        "without_mask_path = '/content/data/without_mask/'\n",
        "data = []\n",
        "\n",
        "# Processing 'with_mask' images\n",
        "for eachImg in with_mask_files:\n",
        "    image = Image.open(with_mask_path + eachImg)\n",
        "    image = image.resize((128, 128))\n",
        "    image = image.convert('RGB') \n",
        "    image = np.array(image)\n",
        "    data.append(image)\n",
        "\n",
        "# Processing 'without_mask' images\n",
        "for eachImg in without_mask_files:\n",
        "    image = Image.open(without_mask_path + eachImg)\n",
        "    image = image.resize((128, 128))\n",
        "    image = image.convert('RGB')\n",
        "    image = np.array(image)\n",
        "    data.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting our lists of data and labels into NumPy arrays.\n",
        "# This is the format TensorFlow/Keras expects.\n",
        "X = np.array(data)\n",
        "Y = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting the data into training and testing sets.\n",
        "# 80% for training, 20% for testing.\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizing the pixel values.\n",
        "# Scaling image data from [0, 255] to [0, 1] helps the model train faster and more effectively.\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step4"
      },
      "source": [
        "### Step 4: Build and Train the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building the CNN model.\n",
        "numClasses = 2\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# First convolutional block: 32 filters, 3x3 kernel, followed by a max pooling layer.\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolutional block: 64 filters. \n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output from the convolutional layers to feed it into the dense layers.\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Adding some dense layers for classification.\n",
        "# Dropout helps prevent overfitting by randomly dropping neurons during training.\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# The final output layer. Sigmoid is used for binary classification.\n",
        "model.add(layers.Dense(numClasses, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compiling the model.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the model for 5 epochs.\n",
        "history = model.fit(x_train, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step5"
      },
      "source": [
        "### Step 5: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}% | Test loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-step6"
      },
      "source": [
        "### Step 6: Test with a Custom Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function to predict on any custom image.\n",
        "\n",
        "class_names = ['Without Mask', 'With Mask'] # Correcting order to match labels (0 and 1)\n",
        "\n",
        "def predict_custom_image(model, img_path):\n",
        "  # 1. Load and resize the image\n",
        "  img = Image.open(img_path)\n",
        "  img = img.resize((128, 128))\n",
        "\n",
        "  # 2. Convert to NumPy array and normalize\n",
        "  img_array = np.array(img)\n",
        "  img_array = img_array / 255.0\n",
        "\n",
        "  # 3. Add a batch dimension, because the model expects it\n",
        "  img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "  # 4. Make a prediction\n",
        "  prediction = model.predict(img_batch)\n",
        "  pred_class_index = np.argmax(prediction[0])\n",
        "  confidence = np.max(prediction[0]) * 100\n",
        "  predicted_class_name = class_names[pred_class_index]\n",
        "\n",
        "  # 5. Display the result\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Predicted: {predicted_class_name}\\nConfidence: {confidence:.2f}%\")\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"The model predicted this image is: {predicted_class_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "imgPath = '/content/custom_datasets/img6.jpg'\n",
        "predict_custom_image(model, imgPath)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
